# Performance analysis of Docker and Firecracker running ML inferences

## Objective

- [ ] To deploy a ML inference on a **Firecracker VM**.
- [ ] To deploy same ML inference model using **Docker**.
- [ ] Compare the performance of the ML inference model in **Docker** and **Firecracker**.
